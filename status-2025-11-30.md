# Status Snapshot – 2025-11-30

## Dienste / Compose (30.11., 21:15)
- LLM: nvidia/NVIDIA-Nemotron-Nano-12B-v2 über vLLM, **max-model-len 131072**, TP=1, GPU0, healthy.
- Embeddings: llama-3.2-nv-embedqa-1b-v2 (1.10.0) ONNX/ORT CUDA, GPU1, healthy.
- Reranker: llama-3.2-nv-rerankqa-1b-v2 ONNX/ORT CUDA, GPU1, healthy (ENABLE_RERANKER=true).
- Vision (page/graphic/table): GPU1, healthy.
- OCR: nemoretriever-ocr-v1 **auf GPU1**, Batch=2, CUDA pool 1024 MB; /v1/health/ready = true.
- NV-Ingest runtime: healthy nach Restart; nutzt OCR HTTP http://nemoretriever-ocr:8000/v1/infer.
- Milvus/MinIO/Etcd, Redis, rag-server, rag-frontend, ingestor-server: up.

## Dienste / Compose (30.11., 20:10) — alt
- LLM: nvidia/NVIDIA-Nemotron-Nano-12B-v2 über vLLM, **max-model-len 131072**, TP=1, GPU0, healthy.
- Embeddings: llama-3.2-nv-embedqa-1b-v2 (1.10.0) ONNX/ORT CUDA, GPU1, healthy.
- Reranker: llama-3.2-nv-rerankqa-1b-v2 ONNX/ORT CUDA, GPU1, healthy (ENABLE_RERANKER=true).
- Vision/OCR (page-/graphic-/table-structure, paddle): GPU1, laufen; table-structure auf GPU1.
- NV-Ingest runtime „unhealthy“ (Healthcheck /v1 vs /v2), funktional OK.
- Milvus/MinIO/Etcd, Redis, rag-server, rag-frontend, ingestor-server: up.

## Dateien
- `deploy/compose/nims.yaml`: LLM max-model-len 131072, TP=1, GPU0; HF caches unter /opt/nim/.cache/huggingface.
- `deploy/compose/docker-compose-rag-server.yaml`: Default-Collection leer (dynamisch per UI); LLM_MAX_TOKENS 4096; VECTOR_DB_TOPK 10; APP_RETRIEVER_TOPK 5; Alias multimodal_data weiterhin auf test9.
- `deploy/compose/.env.single-a100`: GPU-Mapping: LLM=0; Embedding/Reranker=1; Vision/OCR=1; LLM TP bleibt 1-GPU.
- Diagnosen/Pläne unverändert: `docs/reranker-diagnostic.md`, `docs/strategic-plan-reranker.md`, `docs/vsphere-gpu-checklist.md`.

## Hauptprobleme
1) UI-Health-Warnung NV-Ingest (Healthcheck-Endpunkt inkonsistent) – funktional OK.
2) Frühere Kontextlimit-Fehler behoben durch 128k LLM + reduzierte Token-/TopK-Defaults.

## Nächste Schritte
1) UI ggf. Health-Warnungen ignorieren oder Healthcheck-Proxy (/v2→/v1) für NV-Ingest bauen.
2) Bei Bedarf TopK/Max Tokens im UI weiter drosseln, falls lange Kontexte; Backend verkraftet 128k.
3) Engpass beobachten: `watch -n2 nvidia-smi`, `docker ps` Health; GPU1 hostet Vision+Embedding+Reranker.

## Abweichungen zum NVIDIA Blueprint (Stand 30.11., 21:15)
- LLM Modell verkleinert: **NVIDIA-Nemotron-Nano-12B-v2** (statt 49B), Kontext 128k, TP=1 auf GPU0.
- Token-Limits: LLM_MAX_TOKENS=4096 (Frontend zeigte zuvor 512), VECTOR_DB_TOPK=10, APP_RETRIEVER_TOPK=5.
- Collection Handling: Standard-Collection leer; UI/Requests wählen dynamisch (test9 aktiv, 113 Entities).
- GPU-Belegung: GPU0 = LLM; GPU1 = Embedding, Reranker, Vision (page/graphic/table), OCR.
- OCR: paddle deaktiviert; **nemoretriever-ocr-v1** per HTTP genutzt, Batch 2, kleiner CUDA-Pool (1024 MB).
