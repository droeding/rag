{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MCP Server Usage (SSE and stdio)\n",
        "\n",
        "This notebook showcases how to use the NVIDIA RAG MCP server via MCP transports instead of REST APIs. It covers:\n",
        "- Launching the server (SSE and stdio)\n",
        "- Connecting with the MCP Python client\n",
        "- Listing tools\n",
        "- Calling all MCP tools: `generate`, `search`, and `get_summary`\n",
        "\n",
        "You can execute each cell in sequence to test the MCP server APIs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Install Dependencies\n",
        "\n",
        "Purpose:\n",
        "Install the libraries needed to run the MCP server and client locally in this notebook environment.\n",
        "\n",
        "- Ensure your environment has:\n",
        "  - `mcp`, `anyio`, `httpx`, `httpx-sse`, `uvicorn`\n",
        "- If using Workbench/docker, these may already be installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional installation (uncomment to run)\n",
        "# %pip install -qq mcp anyio httpx httpx-sse uvicorn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Setup Base Configuration\n",
        "\n",
        "Purpose:\n",
        "Capture API keys and environment variables (e.g., NVCF) that the server and client will rely on.\n",
        "\n",
        "Configure keys and basic variables used by the rest of this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using key type: <none>\n"
          ]
        }
      ],
      "source": [
        "import os, subprocess, sys, time, atexit\n",
        "from pathlib import Path\n",
        "\n",
        "# Configure keys here for convenience (OPTIONAL: you can also rely on your shell env)\n",
        "# If using NVCF, prefer NVCF_API_KEY and related env vars\n",
        "NVCF_API_KEY = os.environ.get(\"NVCF_API_KEY\", \"\")\n",
        "NVIDIA_API_KEY = os.environ.get(\"NVIDIA_API_KEY\", \"\")\n",
        "NVCF_ORG_ID = os.environ.get(\"NVCF_ORG_ID\", \"\")\n",
        "NVCF_TEAM_ID = os.environ.get(\"NVCF_TEAM_ID\", \"\")\n",
        "NVCF_REGION = os.environ.get(\"NVCF_REGION\", \"\")\n",
        "NVCF_BASE_URL = os.environ.get(\"NVCF_BASE_URL\", \"\")\n",
        "\n",
        "# Choose one usable key for demos (falls back from NVCF to NVIDIA)\n",
        "API_KEY = NVCF_API_KEY or NVIDIA_API_KEY\n",
        "\n",
        "print(\"Using key type:\", \"NVCF\" if NVCF_API_KEY else (\"NVIDIA\" if NVIDIA_API_KEY else \"<none>\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch MCP Server (SSE)\n",
        "\n",
        "Purpose:\n",
        "Start the MCP server locally over SSE so that the client can connect and call tools.\n",
        "\n",
        "This launches the MCP server on `http://127.0.0.1:8000`.\n",
        "- You can pass the API key via `--api-key` or rely on env variables.\n",
        "- The server also accepts `Authorization: Bearer ...` or `x-api-key: ...` headers for SSE requests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kill any process listening on port 8000 to avoid port conflicts\n",
        "import subprocess\n",
        "\n",
        "PORT = 8000\n",
        "print(f\"Kill any process running on port {PORT} to start sse server in the next cell\")\n",
        "\n",
        "# Try fuser first (common on Linux)\n",
        "try:\n",
        "    subprocess.run([\"fuser\", \"-k\", f\"{PORT}/tcp\"], check=False)\n",
        "except FileNotFoundError:\n",
        "    print(\"'fuser' not found, skipping fuser-based cleanup.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error while running fuser: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching: /home/niyati/anaconda3/bin/python -m nvidia_rag.utils.mcp.mcp_server --transport sse --host 127.0.0.1 --port 8000\n",
            "SSE server PID: 1887615\n"
          ]
        }
      ],
      "source": [
        "# Start SSE server in the background\n",
        "sse_proc = None\n",
        "try:\n",
        "    env = dict(os.environ)\n",
        "    # Suppress server-side INFO logs\n",
        "    env[\"LOGLEVEL\"] = \"ERROR\"\n",
        "    if API_KEY:\n",
        "        env[\"NVCF_API_KEY\"] = API_KEY\n",
        "        env.setdefault(\"NVIDIA_API_KEY\", API_KEY)\n",
        "    if NVCF_ORG_ID: env[\"NVCF_ORG_ID\"] = NVCF_ORG_ID\n",
        "    if NVCF_TEAM_ID: env[\"NVCF_TEAM_ID\"] = NVCF_TEAM_ID\n",
        "    if NVCF_REGION: env[\"NVCF_REGION\"] = NVCF_REGION\n",
        "    if NVCF_BASE_URL: env[\"NVCF_BASE_URL\"] = NVCF_BASE_URL\n",
        "\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        \"-m\",\n",
        "        \"nvidia_rag.utils.mcp.mcp_server\",\n",
        "        \"--transport\",\n",
        "        \"sse\",\n",
        "        \"--host\",\n",
        "        \"127.0.0.1\",\n",
        "        \"--port\",\n",
        "        \"8000\",\n",
        "    ]\n",
        "    # Also pass explicit --api-key if available\n",
        "    if API_KEY:\n",
        "        cmd += [\"--api-key\", API_KEY]\n",
        "\n",
        "    print(\"Launching:\", \" \".join(cmd))\n",
        "    sse_proc = subprocess.Popen(cmd, env=env, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    atexit.register(lambda: sse_proc and sse_proc.poll() is None and sse_proc.terminate())\n",
        "    time.sleep(2.0)\n",
        "    print(\"SSE server PID:\", sse_proc.pid)\n",
        "except Exception as e:\n",
        "    print(\"Failed to start SSE server:\", e)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect with MCP Client (SSE), List Tools, and Call MCP Tools\n",
        "\n",
        "Purpose:\n",
        "Verify connectivity by listing available tools and invoking the MCP tools (`generate`, `search`, `get_summary`) using the SSE transport.\n",
        "\n",
        "This uses the `mcp_client.py` CLI to connect over SSE, list tools, and invoke the RAG tools.\n",
        "\n",
        "**Note:** Ensure the SSE server is running from Cell 6 before executing this cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Listing available tools...\n",
            "================================================================================\n",
            "generate: Generate an answer using NVIDIA RAG (optionally with knowledge base). Provide chat messages and optional generation parameters.\n",
            "search: Search the vector database and return citations for a given query.\n",
            "get_summary: Retrieve the pre-generated summary for a document from a collection. Set blocking=true to wait up to timeout seconds for summary generation.\n",
            "\n",
            "================================================================================\n",
            "Calling 'generate' tool...\n",
            "================================================================================\n",
            "{\n",
            "  \"meta\": null,\n",
            "  \"content\": [\n",
            "    {\n",
            "      \"type\": \"text\",\n",
            "      \"text\": \"data: {\\\"id\\\":\\\"f44dcb6f-ba40-4793-bf69-fb5a1a1e413c\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"message\\\":{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":\\\"Collection my_collection does not exist. Ensure a collection is created using POST /collection endpoint first and documents are uploaded using POST /document endpoint\\\"},\\\"delta\\\":{\\\"role\\\":null,\\\"content\\\":\\\"Collection my_collection does not exist. Ensure a collection is created using POST /collection endpoint first and documents are uploaded using POST /document endpoint\\\"},\\\"finish_reason\\\":null}],\\\"model\\\":\\\"nvidia/llama-3.3-nemotron-super-49b-v1.5\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1764738881,\\\"usage\\\":{\\\"total_tokens\\\":0,\\\"prompt_tokens\\\":0,\\\"completion_tokens\\\":0},\\\"citations\\\":{\\\"total_results\\\":0,\\\"results\\\":[]},\\\"metrics\\\":{\\\"rag_ttft_ms\\\":null,\\\"llm_ttft_ms\\\":null,\\\"context_reranker_time_ms\\\":null,\\\"retrieval_time_ms\\\":null,\\\"llm_generation_time_ms\\\":null}}\\n\\ndata: {\\\"id\\\":\\\"f44dcb6f-ba40-4793-bf69-fb5a1a1e413c\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"message\\\":{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":\\\"\\\"},\\\"delta\\\":{\\\"role\\\":null,\\\"content\\\":\\\"\\\"},\\\"finish_reason\\\":\\\"stop\\\"}],\\\"model\\\":\\\"nvidia/llama-3.3-nemotron-super-49b-v1.5\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1764738881,\\\"usage\\\":{\\\"total_tokens\\\":0,\\\"prompt_tokens\\\":0,\\\"completion_tokens\\\":0},\\\"citations\\\":{\\\"total_results\\\":0,\\\"results\\\":[]},\\\"metrics\\\":{\\\"rag_ttft_ms\\\":null,\\\"llm_ttft_ms\\\":0.2410411834716797,\\\"context_reranker_time_ms\\\":null,\\\"retrieval_time_ms\\\":null,\\\"llm_generation_time_ms\\\":0.3437995910644531}}\\n\\n\",\n",
            "      \"annotations\": null,\n",
            "      \"meta\": null\n",
            "    }\n",
            "  ],\n",
            "  \"structuredContent\": {\n",
            "    \"result\": \"data: {\\\"id\\\":\\\"f44dcb6f-ba40-4793-bf69-fb5a1a1e413c\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"message\\\":{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":\\\"Collection my_collection does not exist. Ensure a collection is created using POST /collection endpoint first and documents are uploaded using POST /document endpoint\\\"},\\\"delta\\\":{\\\"role\\\":null,\\\"content\\\":\\\"Collection my_collection does not exist. Ensure a collection is created using POST /collection endpoint first and documents are uploaded using POST /document endpoint\\\"},\\\"finish_reason\\\":null}],\\\"model\\\":\\\"nvidia/llama-3.3-nemotron-super-49b-v1.5\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1764738881,\\\"usage\\\":{\\\"total_tokens\\\":0,\\\"prompt_tokens\\\":0,\\\"completion_tokens\\\":0},\\\"citations\\\":{\\\"total_results\\\":0,\\\"results\\\":[]},\\\"metrics\\\":{\\\"rag_ttft_ms\\\":null,\\\"llm_ttft_ms\\\":null,\\\"context_reranker_time_ms\\\":null,\\\"retrieval_time_ms\\\":null,\\\"llm_generation_time_ms\\\":null}}\\n\\ndata: {\\\"id\\\":\\\"f44dcb6f-ba40-4793-bf69-fb5a1a1e413c\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"message\\\":{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":\\\"\\\"},\\\"delta\\\":{\\\"role\\\":null,\\\"content\\\":\\\"\\\"},\\\"finish_reason\\\":\\\"stop\\\"}],\\\"model\\\":\\\"nvidia/llama-3.3-nemotron-super-49b-v1.5\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1764738881,\\\"usage\\\":{\\\"total_tokens\\\":0,\\\"prompt_tokens\\\":0,\\\"completion_tokens\\\":0},\\\"citations\\\":{\\\"total_results\\\":0,\\\"results\\\":[]},\\\"metrics\\\":{\\\"rag_ttft_ms\\\":null,\\\"llm_ttft_ms\\\":0.2410411834716797,\\\"context_reranker_time_ms\\\":null,\\\"retrieval_time_ms\\\":null,\\\"llm_generation_time_ms\\\":0.3437995910644531}}\\n\\n\"\n",
            "  },\n",
            "  \"isError\": false\n",
            "}\n",
            "\n",
            "================================================================================\n",
            "Calling 'search' tool...\n",
            "================================================================================\n",
            "{\n",
            "  \"meta\": null,\n",
            "  \"content\": [\n",
            "    {\n",
            "      \"type\": \"text\",\n",
            "      \"text\": \"Error executing tool search: Failed to search documents. Collection my_collection does not exist. Ensure a collection is created using POST /collection endpoint first and documents are uploaded using POST /document endpoint\",\n",
            "      \"annotations\": null,\n",
            "      \"meta\": null\n",
            "    }\n",
            "  ],\n",
            "  \"structuredContent\": null,\n",
            "  \"isError\": true\n",
            "}\n",
            "\n",
            "================================================================================\n",
            "Calling 'get_summary' tool...\n",
            "================================================================================\n",
            "{\n",
            "  \"meta\": null,\n",
            "  \"content\": [\n",
            "    {\n",
            "      \"type\": \"text\",\n",
            "      \"text\": \"{\\n  \\\"message\\\": \\\"Summary for document_name.pdf not found. To generate a summary, upload the document with generate_summary=true.\\\",\\n  \\\"status\\\": \\\"NOT_FOUND\\\",\\n  \\\"file_name\\\": \\\"document_name.pdf\\\",\\n  \\\"collection_name\\\": \\\"my_collection\\\"\\n}\",\n",
            "      \"annotations\": null,\n",
            "      \"meta\": null\n",
            "    }\n",
            "  ],\n",
            "  \"structuredContent\": {\n",
            "    \"message\": \"Summary for document_name.pdf not found. To generate a summary, upload the document with generate_summary=true.\",\n",
            "    \"status\": \"NOT_FOUND\",\n",
            "    \"file_name\": \"document_name.pdf\",\n",
            "    \"collection_name\": \"my_collection\"\n",
            "  },\n",
            "  \"isError\": false\n",
            "}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CompletedProcess(args=['/home/niyati/anaconda3/bin/python', '-m', 'nvidia_rag.utils.mcp.mcp_client', 'call', '--transport=sse', '--url=http://127.0.0.1:8000', '--tool=get_summary', '--json-args={\"collection_name\": \"my_collection\", \"file_name\": \"document_name.pdf\", \"blocking\": false, \"timeout\": 60}'], returncode=0)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "import subprocess\n",
        "\n",
        "# SSE connection configuration\n",
        "SSE_URL = \"http://127.0.0.1:8000\"\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Listing available tools...\")\n",
        "print(\"=\"*80)\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    \"-m\",\n",
        "    \"nvidia_rag.utils.mcp.mcp_client\",\n",
        "    \"list\",\n",
        "    \"--transport=sse\",\n",
        "    f\"--url={SSE_URL}\",\n",
        "], stderr=subprocess.DEVNULL)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Calling 'generate' tool...\")\n",
        "print(\"=\"*80)\n",
        "generate_args = json.dumps({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello from SSE demo\"}],\n",
        "    \"collection_name\": \"my_collection\",\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    \"-m\",\n",
        "    \"nvidia_rag.utils.mcp.mcp_client\",\n",
        "    \"call\",\n",
        "    \"--transport=sse\",\n",
        "    f\"--url={SSE_URL}\",\n",
        "    \"--tool=generate\",\n",
        "    f\"--json-args={generate_args}\",\n",
        "], stderr=subprocess.DEVNULL)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Calling 'search' tool...\")\n",
        "print(\"=\"*80)\n",
        "search_args = json.dumps({\n",
        "    \"query\": \"Tell me about Robert Frost's poems\",\n",
        "    \"collection_name\": \"my_collection\",\n",
        "    \"reranker_top_k\": 2,\n",
        "    \"vdb_top_k\": 5,\n",
        "    \"enable_query_rewriting\": False,\n",
        "    \"enable_reranker\": True,\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    \"-m\",\n",
        "    \"nvidia_rag.utils.mcp.mcp_client\",\n",
        "    \"call\",\n",
        "    \"--transport=sse\",\n",
        "    f\"--url={SSE_URL}\",\n",
        "    \"--tool=search\",\n",
        "    f\"--json-args={search_args}\",\n",
        "], stderr=subprocess.DEVNULL)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Calling 'get_summary' tool...\")\n",
        "print(\"=\"*80)\n",
        "summary_args = json.dumps({\n",
        "    \"collection_name\": \"my_collection\",\n",
        "    \"file_name\": \"document_name.pdf\",\n",
        "    \"blocking\": False,\n",
        "    \"timeout\": 60,\n",
        "})\n",
        "subprocess.run([\n",
        "    sys.executable,\n",
        "    \"-m\",\n",
        "    \"nvidia_rag.utils.mcp.mcp_client\",\n",
        "    \"call\",\n",
        "    \"--transport=sse\",\n",
        "    f\"--url={SSE_URL}\",\n",
        "    \"--tool=get_summary\",\n",
        "    f\"--json-args={summary_args}\",\n",
        "], stderr=subprocess.DEVNULL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch MCP Server (streamable_http)\n",
        "\n",
        "Purpose:\n",
        "Start the MCP server locally using the **streamable_http** transport so that the client can connect and call tools.\n",
        "\n",
        "This launches the MCP server with FastMCP's streamable-http support:\n",
        "- Uses your configured API key (NVCF or NVIDIA) from the environment.\n",
        "- Runs `nvidia_rag.utils.mcp.mcp_server` with `--transport streamable_http`.\n",
        "\n",
        "**Note:** Run this cell once before executing the subsequent streamable_http client cells that list and call MCP tools.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kill any process listening on port 8000 to avoid port conflicts\n",
        "import subprocess\n",
        "\n",
        "PORT = 8000\n",
        "print(f\"Kill any process running on port {PORT} to start streamable_http server in the next cell\")\n",
        "\n",
        "# Try fuser first (common on Linux)\n",
        "try:\n",
        "    subprocess.run([\"fuser\", \"-k\", f\"{PORT}/tcp\"], check=False)\n",
        "except FileNotFoundError:\n",
        "    print(\"'fuser' not found, skipping fuser-based cleanup.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error while running fuser: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching streamable_http server: /home/niyati/anaconda3/bin/python -m nvidia_rag.utils.mcp.mcp_server --transport streamable_http\n",
            "streamable_http server PID: 1907814\n"
          ]
        }
      ],
      "source": [
        "# Start streamable_http MCP server in the background\n",
        "stream_proc = None\n",
        "try:\n",
        "    env = dict(os.environ)\n",
        "    env[\"LOGLEVEL\"] = \"ERROR\"\n",
        "    if API_KEY:\n",
        "        env[\"NVCF_API_KEY\"] = API_KEY\n",
        "        env.setdefault(\"NVIDIA_API_KEY\", API_KEY)\n",
        "    if NVCF_ORG_ID: env[\"NVCF_ORG_ID\"] = NVCF_ORG_ID\n",
        "    if NVCF_TEAM_ID: env[\"NVCF_TEAM_ID\"] = NVCF_TEAM_ID\n",
        "    if NVCF_REGION: env[\"NVCF_REGION\"] = NVCF_REGION\n",
        "    if NVCF_BASE_URL: env[\"NVCF_BASE_URL\"] = NVCF_BASE_URL\n",
        "\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        \"-m\",\n",
        "        \"nvidia_rag.utils.mcp.mcp_server\",\n",
        "        \"--transport\",\n",
        "        \"streamable_http\",\n",
        "    ]\n",
        "    if API_KEY:\n",
        "        cmd += [\"--api-key\", API_KEY]\n",
        "\n",
        "    print(\"Launching streamable_http server:\", \" \".join(cmd))\n",
        "    stream_proc = subprocess.Popen(cmd, env=env, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    atexit.register(lambda: stream_proc and stream_proc.poll() is None and stream_proc.terminate())\n",
        "    time.sleep(2.0)\n",
        "    print(\"streamable_http server PID:\", stream_proc.pid)\n",
        "except Exception as e:\n",
        "    print(\"Failed to start streamable_http server:\", e)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect with MCP Client (streamable_http), List Tools, and Call MCP Tools\n",
        "\n",
        "Purpose:\n",
        "Verify connectivity by listing available tools and invoking the MCP tools (`generate`, `search`, `get_summary`) using the streamable_http transport.\n",
        "\n",
        "This uses the `mcp_client.py` CLI to connect over streamable_http, list tools, and invoke the RAG tools.\n",
        "\n",
        "**Note:** Ensure the streamable_http server is running from the cell above before executing this cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "[streamable_http] Listing available tools...\n",
            "================================================================================\n",
            "generate: Generate an answer using NVIDIA RAG (optionally with knowledge base). Provide chat messages and optional generation parameters.\n",
            "search: Search the vector database and return citations for a given query.\n",
            "get_summary: Retrieve the pre-generated summary for a document from a collection. Set blocking=true to wait up to timeout seconds for summary generation.\n",
            "\n",
            "================================================================================\n",
            "[streamable_http] Calling 'generate' tool...\n",
            "================================================================================\n",
            "{\n",
            "  \"meta\": null,\n",
            "  \"content\": [\n",
            "    {\n",
            "      \"type\": \"text\",\n",
            "      \"text\": \"data: {\\\"id\\\":\\\"4d45f450-1f6a-4b9c-bb2e-983dea8d65b8\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"message\\\":{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":\\\"Collection my_collection does not exist. Ensure a collection is created using POST /collection endpoint first and documents are uploaded using POST /document endpoint\\\"},\\\"delta\\\":{\\\"role\\\":null,\\\"content\\\":\\\"Collection my_collection does not exist. Ensure a collection is created using POST /collection endpoint first and documents are uploaded using POST /document endpoint\\\"},\\\"finish_reason\\\":null}],\\\"model\\\":\\\"nvidia/llama-3.3-nemotron-super-49b-v1.5\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1764739203,\\\"usage\\\":{\\\"total_tokens\\\":0,\\\"prompt_tokens\\\":0,\\\"completion_tokens\\\":0},\\\"citations\\\":{\\\"total_results\\\":0,\\\"results\\\":[]},\\\"metrics\\\":{\\\"rag_ttft_ms\\\":null,\\\"llm_ttft_ms\\\":null,\\\"context_reranker_time_ms\\\":null,\\\"retrieval_time_ms\\\":null,\\\"llm_generation_time_ms\\\":null}}\\n\\ndata: {\\\"id\\\":\\\"4d45f450-1f6a-4b9c-bb2e-983dea8d65b8\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"message\\\":{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":\\\"\\\"},\\\"delta\\\":{\\\"role\\\":null,\\\"content\\\":\\\"\\\"},\\\"finish_reason\\\":\\\"stop\\\"}],\\\"model\\\":\\\"nvidia/llama-3.3-nemotron-super-49b-v1.5\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1764739203,\\\"usage\\\":{\\\"total_tokens\\\":0,\\\"prompt_tokens\\\":0,\\\"completion_tokens\\\":0},\\\"citations\\\":{\\\"total_results\\\":0,\\\"results\\\":[]},\\\"metrics\\\":{\\\"rag_ttft_ms\\\":null,\\\"llm_ttft_ms\\\":0.28514862060546875,\\\"context_reranker_time_ms\\\":null,\\\"retrieval_time_ms\\\":null,\\\"llm_generation_time_ms\\\":0.3528594970703125}}\\n\\n\",\n",
            "      \"annotations\": null,\n",
            "      \"meta\": null\n",
            "    }\n",
            "  ],\n",
            "  \"structuredContent\": {\n",
            "    \"result\": \"data: {\\\"id\\\":\\\"4d45f450-1f6a-4b9c-bb2e-983dea8d65b8\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"message\\\":{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":\\\"Collection my_collection does not exist. Ensure a collection is created using POST /collection endpoint first and documents are uploaded using POST /document endpoint\\\"},\\\"delta\\\":{\\\"role\\\":null,\\\"content\\\":\\\"Collection my_collection does not exist. Ensure a collection is created using POST /collection endpoint first and documents are uploaded using POST /document endpoint\\\"},\\\"finish_reason\\\":null}],\\\"model\\\":\\\"nvidia/llama-3.3-nemotron-super-49b-v1.5\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1764739203,\\\"usage\\\":{\\\"total_tokens\\\":0,\\\"prompt_tokens\\\":0,\\\"completion_tokens\\\":0},\\\"citations\\\":{\\\"total_results\\\":0,\\\"results\\\":[]},\\\"metrics\\\":{\\\"rag_ttft_ms\\\":null,\\\"llm_ttft_ms\\\":null,\\\"context_reranker_time_ms\\\":null,\\\"retrieval_time_ms\\\":null,\\\"llm_generation_time_ms\\\":null}}\\n\\ndata: {\\\"id\\\":\\\"4d45f450-1f6a-4b9c-bb2e-983dea8d65b8\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"message\\\":{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":\\\"\\\"},\\\"delta\\\":{\\\"role\\\":null,\\\"content\\\":\\\"\\\"},\\\"finish_reason\\\":\\\"stop\\\"}],\\\"model\\\":\\\"nvidia/llama-3.3-nemotron-super-49b-v1.5\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1764739203,\\\"usage\\\":{\\\"total_tokens\\\":0,\\\"prompt_tokens\\\":0,\\\"completion_tokens\\\":0},\\\"citations\\\":{\\\"total_results\\\":0,\\\"results\\\":[]},\\\"metrics\\\":{\\\"rag_ttft_ms\\\":null,\\\"llm_ttft_ms\\\":0.28514862060546875,\\\"context_reranker_time_ms\\\":null,\\\"retrieval_time_ms\\\":null,\\\"llm_generation_time_ms\\\":0.3528594970703125}}\\n\\n\"\n",
            "  },\n",
            "  \"isError\": false\n",
            "}\n",
            "\n",
            "================================================================================\n",
            "[streamable_http] Calling 'search' tool...\n",
            "================================================================================\n",
            "{\n",
            "  \"meta\": null,\n",
            "  \"content\": [\n",
            "    {\n",
            "      \"type\": \"text\",\n",
            "      \"text\": \"Error executing tool search: Failed to search documents. Collection my_collection does not exist. Ensure a collection is created using POST /collection endpoint first and documents are uploaded using POST /document endpoint\",\n",
            "      \"annotations\": null,\n",
            "      \"meta\": null\n",
            "    }\n",
            "  ],\n",
            "  \"structuredContent\": null,\n",
            "  \"isError\": true\n",
            "}\n",
            "\n",
            "================================================================================\n",
            "[streamable_http] Calling 'get_summary' tool...\n",
            "================================================================================\n",
            "{\n",
            "  \"meta\": null,\n",
            "  \"content\": [\n",
            "    {\n",
            "      \"type\": \"text\",\n",
            "      \"text\": \"{\\n  \\\"message\\\": \\\"Summary for document_name.pdf not found. To generate a summary, upload the document with generate_summary=true.\\\",\\n  \\\"status\\\": \\\"NOT_FOUND\\\",\\n  \\\"file_name\\\": \\\"document_name.pdf\\\",\\n  \\\"collection_name\\\": \\\"my_collection\\\"\\n}\",\n",
            "      \"annotations\": null,\n",
            "      \"meta\": null\n",
            "    }\n",
            "  ],\n",
            "  \"structuredContent\": {\n",
            "    \"message\": \"Summary for document_name.pdf not found. To generate a summary, upload the document with generate_summary=true.\",\n",
            "    \"status\": \"NOT_FOUND\",\n",
            "    \"file_name\": \"document_name.pdf\",\n",
            "    \"collection_name\": \"my_collection\"\n",
            "  },\n",
            "  \"isError\": false\n",
            "}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CompletedProcess(args=['/home/niyati/anaconda3/bin/python', '-m', 'nvidia_rag.utils.mcp.mcp_client', 'call', '--transport', 'streamable_http', '--url', 'http://127.0.0.1:8000', '--tool', 'get_summary', '--json-args', '{\"collection_name\": \"my_collection\", \"file_name\": \"document_name.pdf\", \"blocking\": false, \"timeout\": 60}'], returncode=0)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "import subprocess\n",
        "\n",
        "# Streamable HTTP connection configuration\n",
        "STREAM_URL = \"http://127.0.0.1:8000\"  # base URL; client normalizes to /mcp\n",
        "\n",
        "common_args = [sys.executable, \"-m\", \"nvidia_rag.utils.mcp.mcp_client\"]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"[streamable_http] Listing available tools...\")\n",
        "print(\"=\"*80)\n",
        "list_args = common_args + [\n",
        "    \"list\",\n",
        "    \"--transport\",\n",
        "    \"streamable_http\",\n",
        "    \"--url\",\n",
        "    STREAM_URL,\n",
        "]\n",
        "if API_KEY:\n",
        "    list_args += [\"--header\", f\"Authorization: Bearer {API_KEY}\"]\n",
        "subprocess.run(list_args, stderr=subprocess.DEVNULL)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"[streamable_http] Calling 'generate' tool...\")\n",
        "print(\"=\"*80)\n",
        "generate_args_payload = json.dumps({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello from streamable_http demo\"}],\n",
        "    \"collection_name\": \"my_collection\",\n",
        "})\n",
        "call_generate_args = common_args + [\n",
        "    \"call\",\n",
        "    \"--transport\",\n",
        "    \"streamable_http\",\n",
        "    \"--url\",\n",
        "    STREAM_URL,\n",
        "    \"--tool\",\n",
        "    \"generate\",\n",
        "    \"--json-args\",\n",
        "    generate_args_payload,\n",
        "]\n",
        "if API_KEY:\n",
        "    call_generate_args += [\"--header\", f\"Authorization: Bearer {API_KEY}\"]\n",
        "subprocess.run(call_generate_args, stderr=subprocess.DEVNULL)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"[streamable_http] Calling 'search' tool...\")\n",
        "print(\"=\"*80)\n",
        "search_args_payload = json.dumps({\n",
        "    \"query\": \"Tell me about Robert Frost's poems\",\n",
        "    \"collection_name\": \"my_collection\",\n",
        "    \"reranker_top_k\": 2,\n",
        "    \"vdb_top_k\": 5,\n",
        "    \"enable_query_rewriting\": False,\n",
        "    \"enable_reranker\": True,\n",
        "})\n",
        "call_search_args = common_args + [\n",
        "    \"call\",\n",
        "    \"--transport\",\n",
        "    \"streamable_http\",\n",
        "    \"--url\",\n",
        "    STREAM_URL,\n",
        "    \"--tool\",\n",
        "    \"search\",\n",
        "    \"--json-args\",\n",
        "    search_args_payload,\n",
        "]\n",
        "if API_KEY:\n",
        "    call_search_args += [\"--header\", f\"Authorization: Bearer {API_KEY}\"]\n",
        "subprocess.run(call_search_args, stderr=subprocess.DEVNULL)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"[streamable_http] Calling 'get_summary' tool...\")\n",
        "print(\"=\"*80)\n",
        "summary_args_payload = json.dumps({\n",
        "    \"collection_name\": \"my_collection\",\n",
        "    \"file_name\": \"document_name.pdf\",\n",
        "    \"blocking\": False,\n",
        "    \"timeout\": 60,\n",
        "})\n",
        "call_summary_args = common_args + [\n",
        "    \"call\",\n",
        "    \"--transport\",\n",
        "    \"streamable_http\",\n",
        "    \"--url\",\n",
        "    STREAM_URL,\n",
        "    \"--tool\",\n",
        "    \"get_summary\",\n",
        "    \"--json-args\",\n",
        "    summary_args_payload,\n",
        "]\n",
        "if API_KEY:\n",
        "    call_summary_args += [\"--header\", f\"Authorization: Bearer {API_KEY}\"]\n",
        "subprocess.run(call_summary_args, stderr=subprocess.DEVNULL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch Server via Client (stdio), List Tools, and Call MCP Tools\n",
        "\n",
        "Purpose:\n",
        "Demonstrate launching the server via stdio transport, listing tools, and calling the MCP tools (`generate`, `search`, `get_summary`), forwarding the API key to the child server process.\n",
        "\n",
        "This uses the `mcp_client.py` CLI to launch the server via stdio and invoke the RAG tools. The API key is forwarded via server arguments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Listing available tools via stdio...\n",
            "================================================================================\n",
            "generate: Generate an answer using NVIDIA RAG (optionally with knowledge base). Provide chat messages and optional generation parameters.\n",
            "search: Search the vector database and return citations for a given query.\n",
            "get_summary: Retrieve the pre-generated summary for a document from a collection. Set blocking=true to wait up to timeout seconds for summary generation.\n",
            "\n",
            "================================================================================\n",
            "Calling 'generate' tool via stdio...\n",
            "================================================================================\n",
            "{\n",
            "  \"meta\": null,\n",
            "  \"content\": [\n",
            "    {\n",
            "      \"type\": \"text\",\n",
            "      \"text\": \"data: {\\\"id\\\":\\\"82284346-d3af-450b-bf6f-fff2537f5034\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"message\\\":{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":\\\"Collection my_collection does not exist. Ensure a collection is created using POST /collection endpoint first and documents are uploaded using POST /document endpoint\\\"},\\\"delta\\\":{\\\"role\\\":null,\\\"content\\\":\\\"Collection my_collection does not exist. Ensure a collection is created using POST /collection endpoint first and documents are uploaded using POST /document endpoint\\\"},\\\"finish_reason\\\":null}],\\\"model\\\":\\\"nvidia/llama-3.3-nemotron-super-49b-v1.5\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1764739224,\\\"usage\\\":{\\\"total_tokens\\\":0,\\\"prompt_tokens\\\":0,\\\"completion_tokens\\\":0},\\\"citations\\\":{\\\"total_results\\\":0,\\\"results\\\":[]},\\\"metrics\\\":{\\\"rag_ttft_ms\\\":null,\\\"llm_ttft_ms\\\":null,\\\"context_reranker_time_ms\\\":null,\\\"retrieval_time_ms\\\":null,\\\"llm_generation_time_ms\\\":null}}\\n\\ndata: {\\\"id\\\":\\\"82284346-d3af-450b-bf6f-fff2537f5034\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"message\\\":{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":\\\"\\\"},\\\"delta\\\":{\\\"role\\\":null,\\\"content\\\":\\\"\\\"},\\\"finish_reason\\\":\\\"stop\\\"}],\\\"model\\\":\\\"nvidia/llama-3.3-nemotron-super-49b-v1.5\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1764739224,\\\"usage\\\":{\\\"total_tokens\\\":0,\\\"prompt_tokens\\\":0,\\\"completion_tokens\\\":0},\\\"citations\\\":{\\\"total_results\\\":0,\\\"results\\\":[]},\\\"metrics\\\":{\\\"rag_ttft_ms\\\":null,\\\"llm_ttft_ms\\\":0.438690185546875,\\\"context_reranker_time_ms\\\":null,\\\"retrieval_time_ms\\\":null,\\\"llm_generation_time_ms\\\":0.7190704345703125}}\\n\\n\",\n",
            "      \"annotations\": null,\n",
            "      \"meta\": null\n",
            "    }\n",
            "  ],\n",
            "  \"structuredContent\": {\n",
            "    \"result\": \"data: {\\\"id\\\":\\\"82284346-d3af-450b-bf6f-fff2537f5034\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"message\\\":{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":\\\"Collection my_collection does not exist. Ensure a collection is created using POST /collection endpoint first and documents are uploaded using POST /document endpoint\\\"},\\\"delta\\\":{\\\"role\\\":null,\\\"content\\\":\\\"Collection my_collection does not exist. Ensure a collection is created using POST /collection endpoint first and documents are uploaded using POST /document endpoint\\\"},\\\"finish_reason\\\":null}],\\\"model\\\":\\\"nvidia/llama-3.3-nemotron-super-49b-v1.5\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1764739224,\\\"usage\\\":{\\\"total_tokens\\\":0,\\\"prompt_tokens\\\":0,\\\"completion_tokens\\\":0},\\\"citations\\\":{\\\"total_results\\\":0,\\\"results\\\":[]},\\\"metrics\\\":{\\\"rag_ttft_ms\\\":null,\\\"llm_ttft_ms\\\":null,\\\"context_reranker_time_ms\\\":null,\\\"retrieval_time_ms\\\":null,\\\"llm_generation_time_ms\\\":null}}\\n\\ndata: {\\\"id\\\":\\\"82284346-d3af-450b-bf6f-fff2537f5034\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"message\\\":{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":\\\"\\\"},\\\"delta\\\":{\\\"role\\\":null,\\\"content\\\":\\\"\\\"},\\\"finish_reason\\\":\\\"stop\\\"}],\\\"model\\\":\\\"nvidia/llama-3.3-nemotron-super-49b-v1.5\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1764739224,\\\"usage\\\":{\\\"total_tokens\\\":0,\\\"prompt_tokens\\\":0,\\\"completion_tokens\\\":0},\\\"citations\\\":{\\\"total_results\\\":0,\\\"results\\\":[]},\\\"metrics\\\":{\\\"rag_ttft_ms\\\":null,\\\"llm_ttft_ms\\\":0.438690185546875,\\\"context_reranker_time_ms\\\":null,\\\"retrieval_time_ms\\\":null,\\\"llm_generation_time_ms\\\":0.7190704345703125}}\\n\\n\"\n",
            "  },\n",
            "  \"isError\": false\n",
            "}\n",
            "\n",
            "================================================================================\n",
            "Calling 'search' tool via stdio...\n",
            "================================================================================\n",
            "{\n",
            "  \"meta\": null,\n",
            "  \"content\": [\n",
            "    {\n",
            "      \"type\": \"text\",\n",
            "      \"text\": \"Error executing tool search: Failed to search documents. Collection my_collection does not exist. Ensure a collection is created using POST /collection endpoint first and documents are uploaded using POST /document endpoint\",\n",
            "      \"annotations\": null,\n",
            "      \"meta\": null\n",
            "    }\n",
            "  ],\n",
            "  \"structuredContent\": null,\n",
            "  \"isError\": true\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import subprocess\n",
        "\n",
        "# Configure stdio command and arguments\n",
        "STDIO_CMD = sys.executable\n",
        "STDIO_ARGS = \"-m nvidia_rag.utils.mcp.mcp_server\"\n",
        "if API_KEY:\n",
        "    STDIO_ARGS += f\" --api-key {API_KEY}\"\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Listing available tools via stdio...\")\n",
        "print(\"=\"*80)\n",
        "subprocess.run([sys.executable, \"-m\", \"nvidia_rag.utils.mcp.mcp_client\", \"list\", \"--transport=stdio\", f\"--command={STDIO_CMD}\", f\"--args={STDIO_ARGS}\"], stderr=subprocess.DEVNULL)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Calling 'generate' tool via stdio...\")\n",
        "print(\"=\"*80)\n",
        "generate_args = json.dumps({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello from stdio demo\"}],\n",
        "    \"collection_name\": \"my_collection\",\n",
        "})\n",
        "subprocess.run([sys.executable, \"-m\", \"nvidia_rag.utils.mcp.mcp_client\", \"call\", \"--transport=stdio\", f\"--command={STDIO_CMD}\", f\"--args={STDIO_ARGS}\", \"--tool=generate\", f\"--json-args={generate_args}\"], stderr=subprocess.DEVNULL)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Calling 'search' tool via stdio...\")\n",
        "print(\"=\"*80)\n",
        "search_args = json.dumps({\n",
        "    \"query\": \"Tell me about Robert Frost's poems\",\n",
        "    \"collection_name\": \"my_collection\",\n",
        "    \"reranker_top_k\": 2,\n",
        "    \"vdb_top_k\": 5,\n",
        "    \"enable_query_rewriting\": False,\n",
        "    \"enable_reranker\": True,\n",
        "})\n",
        "subprocess.run([sys.executable, \"-m\", \"nvidia_rag.utils.mcp.mcp_client\", \"call\", \"--transport=stdio\", f\"--command={STDIO_CMD}\", f\"--args={STDIO_ARGS}\", \"--tool=search\", f\"--json-args={search_args}\"], stderr=subprocess.DEVNULL)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Calling 'get_summary' tool via stdio...\")\n",
        "print(\"=\"*80)\n",
        "summary_args = json.dumps({\n",
        "    \"collection_name\": \"my_collection\",\n",
        "    \"file_name\": \"document_name.pdf\",\n",
        "    \"blocking\": False,\n",
        "    \"timeout\": 60,\n",
        "})\n",
        "subprocess.run([sys.executable, \"-m\", \"nvidia_rag.utils.mcp.mcp_client\", \"call\", \"--transport=stdio\", f\"--command={STDIO_CMD}\", f\"--args={STDIO_ARGS}\", \"--tool=get_summary\", f\"--json-args={summary_args}\"], stderr=subprocess.DEVNULL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup & Troubleshooting\n",
        "\n",
        "Purpose:\n",
        "Wrap up the session, stop background processes, and provide guidance for common errors (401/404) and environment/version mismatches.\n",
        "\n",
        "- To stop the SSE server started above, restart the kernel or run the cell that terminates the `sse_proc`.\n",
        "- If you see 401 Unauthorized:\n",
        "  - Ensure your API key is valid and has access to the configured model endpoint.\n",
        "  - For NVCF deployments, set `NVCF_API_KEY`, and usually `NVCF_ORG_ID`, `NVCF_TEAM_ID`, `NVCF_REGION`, `NVCF_BASE_URL`.\n",
        "  - In stdio mode, forward env with `--env VAR=...` or use the server `--api-key` flag.\n",
        "- If SSE returns 404, ensure you're connecting to the base URL (the client probes standard SSE endpoints).\n",
        "- Ensure versions of `mcp`, `anyio`, and `uvicorn` match your environment constraints.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
