{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf6ca4e",
   "metadata": {},
   "source": [
    "# Document Summarization Customization Guide\n",
    "\n",
    "This notebook demonstrates how to customize the document summarization feature in NVIDIA RAG.\n",
    "\n",
    "## Two Modes of Operation\n",
    "\n",
    "- **Library Mode**: Programmatic configuration changes in Python notebooks/scripts\n",
    "- **Docker Mode**: Configuration via environment variables and config files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b134dbf9",
   "metadata": {},
   "source": [
    "## üìä Summarization Pipeline Architecture\n",
    "\n",
    "The diagram below shows how document summarization integrates into the complete RAG pipeline:\n",
    "\n",
    "![Summarization Pipeline Architecture](../docs/assets/summarization_flow_diagram.png)\n",
    "\n",
    "The summarization workflow that this notebook focuses on. You'll learn to customize:\n",
    "\n",
    "- **Page Filtering**: Select specific pages using ranges, negative indexing, or even/odd patterns\n",
    "- **Shallow vs Full Extraction**: Fast text-only OR comprehensive multimodal processing\n",
    "- **Summarization Strategy**: Choose between Single (fastest), Hierarchical (balanced), or Iterative (best quality - default)\n",
    "    - **Single**: Merge all content, chunk by configured size, and summarize only the first chunk (fastest, one LLM call)\n",
    "    - **Hierarchical**: Tree-based summarization - summarize all chunks, merge summaries until they fit chunk size, repeat recursively until reaching one final summary (balanced speed/quality)\n",
    "    - **Iterative (default)**: Process chunks sequentially with context refinement from previous summaries (best quality, N sequential LLM calls)\n",
    "- **Token-based Chunking**: 9000 tokens per chunk with 400 token overlap\n",
    "- **Real-time Status Tracking**: Monitor progress via Redis with chunk-level updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc56f4f",
   "metadata": {},
   "source": [
    "## Part 1: Library Mode "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ee4727",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f389a897",
   "metadata": {},
   "source": [
    "### Step 1: Setup before using library mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adbc9f2",
   "metadata": {},
   "source": [
    "## Installation guide for python package\n",
    "\n",
    "Before running the cells below, follow these steps in your terminal from the project root directory to install the python package in your environment and launch this notebook:\n",
    "\n",
    "> **Note**: Python version **3.12 or higher** is supported.\n",
    "\n",
    "```bash\n",
    "# 1. Install Python >= 3.12 (e.g., Python 3.13) and its development headers\n",
    "    sudo add-apt-repository ppa:deadsnakes/ppa\n",
    "    sudo apt update\n",
    "    sudo apt install python3.12\n",
    "    sudo apt-get install python3.12-dev\n",
    "\n",
    "# 2. Install uv\n",
    "Follow instruction from https://docs.astral.sh/uv/getting-started/installation/\n",
    "\n",
    "# 3. Create a virtual environment with a supported Python version (>= 3.12)\n",
    "uv venv --python=python3.12\n",
    "\n",
    "# 2. Activate the virtual environment\n",
    "source .venv/bin/activate\n",
    "\n",
    "# 3. (Option 1) Build the wheel from source and install the Nvidia RAG wheel\n",
    "uv build\n",
    "uv pip install dist/nvidia_rag-2.4.0.dev0-py3-none-any.whl[all]\n",
    "\n",
    "# 4. (Option 2) Install the package in editable (development) mode from source\n",
    "uv pip install -e .[all]\n",
    "\n",
    "# 5. (Option 3) Install the prebuilt wheel file from pypi. This does not require you to clone the repo.\n",
    "uv pip install nvidia-rag[all]\n",
    "\n",
    "# 5. Start the notebook server and open this notebook in browser \n",
    "uv pip install jupyterlab\n",
    "jupyter lab --allow-root --ip=0.0.0.0 --NotebookApp.token='' --port=8889 --no-browser &\n",
    "Open http://<workstation_ip>:8889/lab/tree/notebooks\n",
    "\n",
    "# 6. Optional: Install just RAG and Ingestor dependencies\n",
    "uv pip install dist/nvidia_rag-2.4.0.dev0-py3-none-any.whl[rag]\n",
    "uv pip install dist/nvidia_rag-2.4.0.dev0-py3-none-any.whl[ingest]\n",
    "```\n",
    "\n",
    "##### üìù **Note:**\n",
    "\n",
    "- Installing with `uv pip install -e .[all]` allows you to make live edits to the `nvidia_rag` source code and have those changes reflected without reinstalling the package.\n",
    "- **After making changes to the source code, you need to:\n",
    "  - Restart the kernel of your notebook server\n",
    "  - Re-execute the cells `Setup the default configurations` under `Setting up the dependencies` and `Import the packages` under `API usage examples`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdd2bd9",
   "metadata": {},
   "source": [
    "#### Verify the installation\n",
    "The location of the package shown in the output of this command should be inside the virtual environment.\n",
    "\n",
    "Location: `<workspace_path>/rag/.venv/lib/python3.12/site-packages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c40dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip show nvidia_rag | grep Location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb24e12b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff09d666",
   "metadata": {},
   "source": [
    "## Setting up the dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdd89dc",
   "metadata": {},
   "source": [
    "After the environment for the python package is setup we now launch all the dependent services and NIMs the pipeline depends on.\n",
    "Fulfill the [prerequisites here](../docs/deploy-docker-self-hosted.md) to setup docker on your system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf4f5f",
   "metadata": {},
   "source": [
    "### 1. Setup the default configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5162b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install python-dotenv\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec509cc9",
   "metadata": {},
   "source": [
    "Provide your NGC_API_KEY after executing the cell below. You can obtain a key by following steps [here](../docs/api-key.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f81ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset if needed\n",
    "if os.environ.get(\"NGC_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NGC_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    candidate_api_key = getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert candidate_api_key.startswith(\"nvapi-\"), (\n",
    "        f\"{candidate_api_key[:5]}... is not a valid key\"\n",
    "    )\n",
    "    os.environ[\"NGC_API_KEY\"] = candidate_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dec262",
   "metadata": {},
   "source": [
    "Login to nvcr.io which is needed for pulling the containers of dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec18c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe12ba6",
   "metadata": {},
   "source": [
    "Load the default values for all the configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e89c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=\".env_library\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f04fd1d",
   "metadata": {},
   "source": [
    "*üí° **Tip:***: You can override any default values of configurations defined in `.env_library` at runtime by using `os.environ` in the notebook. Reimport the `nvidia_rag` package and restart the  Nvidia Ingest runtime to take in the updated configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5296fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# os.environ[\"ENV_VAR_NAME\"]=\"ENV_VAR_VALUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ac9b0",
   "metadata": {},
   "source": [
    "### 2. Setup the Milvus vector DB services\n",
    "By default milvus uses GPU Indexing. Ensure you have provided correct GPU ID.\n",
    "Note: If you don't have a GPU available, you can switch to CPU-only Milvus by following the instructions in [milvus-configuration.md](../docs/milvus-configuration.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fa2f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VECTORSTORE_GPU_DEVICE_ID\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f ../deploy/compose/vectordb.yaml up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969aa2c",
   "metadata": {},
   "source": [
    "### 3. Setup the NIMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99abdce4",
   "metadata": {},
   "source": [
    "#### Option 1: Deploy on-prem models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb2ea98",
   "metadata": {},
   "source": [
    "Move to Option 2 if you are interested in using cloud models.\n",
    "\n",
    "Ensure you meet [the hardware requirements](../docs/support-matrix.md). By default the NIMs are configured to use 2xH100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b14c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model cache directory\n",
    "!mkdir -p ~/.cache/model-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50299ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the MODEL_DIRECTORY environment variable in the Python kernel\n",
    "import os\n",
    "\n",
    "os.environ[\"MODEL_DIRECTORY\"] = os.path.expanduser(\"~/.cache/model-cache\")\n",
    "print(\"MODEL_DIRECTORY set to:\", os.environ[\"MODEL_DIRECTORY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f577f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GPU IDs for the various microservices if needed\n",
    "os.environ[\"EMBEDDING_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"RANKING_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"YOLOX_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"YOLOX_GRAPHICS_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"YOLOX_TABLE_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"OCR_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"LLM_MS_GPU_ID\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c621259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è Deploying NIMs - This may take a while as models download. If kernel times out, just rerun this cell.\n",
    "!USERID=$(id -u) docker compose -f ../deploy/compose/nims.yaml up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e0f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch the status of running containers (run this cell repeatedly or in a terminal)\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fed48c0",
   "metadata": {},
   "source": [
    "Ensure all the below are running and healthy before proceeding further\n",
    "```output\n",
    "NAMES                           STATUS\n",
    "nemoretriever-ranking-ms        Up ... (healthy)\n",
    "compose-page-elements-1         Up ...\n",
    "compose-paddle-1                Up ...\n",
    "compose-graphic-elements-1      Up ...\n",
    "compose-table-structure-1       Up ...\n",
    "nemoretriever-embedding-ms      Up ... (healthy)\n",
    "nim-llm-ms                      Up ... (healthy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac5b7a",
   "metadata": {},
   "source": [
    "#### Option 2: Using Nvidia Hosted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dad65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia_rag.utils.configuration import NvidiaRAGConfig\n",
    "\n",
    "# Get the config object for runtime modifications\n",
    "# We'll override the default localhost URLs to use NVIDIA hosted APIs\n",
    "config = NvidiaRAGConfig.from_yaml(\"config.yaml\")\n",
    "\n",
    "# Configure models to use NVIDIA hosted APIs\n",
    "config.llm.model_name = \"nvidia/llama-3.3-nemotron-super-49b-v1.5\"\n",
    "config.llm.server_url = \"\"  # Empty = use NVIDIA hosted API\n",
    "\n",
    "config.embeddings.model_name = \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n",
    "config.embeddings.server_url = \"\"  # Empty = use NVIDIA hosted API\n",
    "\n",
    "config.ranking.model_name = \"nvidia/llama-3.2-nv-rerankqa-1b-v2\"\n",
    "config.ranking.server_url = \"https://ai.api.nvidia.com/v1/retrieval/nvidia/llama-3_2-nv-rerankqa-1b-v2/reranking/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d529315",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OCR_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/baidu/paddleocr\"\n",
    "os.environ[\"OCR_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_HTTP_ENDPOINT\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v2\"\n",
    ")\n",
    "os.environ[\"YOLOX_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-graphic-elements-v1\"\n",
    ")\n",
    "os.environ[\"YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-table-structure-v1\"\n",
    ")\n",
    "os.environ[\"YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL\"] = \"http\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b176dc",
   "metadata": {},
   "source": [
    "### 4. Setup the Nvidia Ingest runtime and redis service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be355c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f ../deploy/compose/docker-compose-ingestor-server.yaml up nv-ingest-ms-runtime redis -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc0ef32",
   "metadata": {},
   "source": [
    "### 5. Load optional profiles if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1fca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load accuracy profile\n",
    "# load_dotenv(dotenv_path='../deploy/compose/accuracy_profile.env', override=True)\n",
    "\n",
    "# OR load perf profile\n",
    "# load_dotenv(dotenv_path='../deploy/compose/perf_profile.env', override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1928cf",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary Feature usage example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b33ced",
   "metadata": {},
   "source": [
    "After setting up the python package and starting all dependent services, finally we can execute some snippets showcasing all summary related functionalities offered by the `nvidia_rag` package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f067c70",
   "metadata": {},
   "source": [
    "## Set logging level\n",
    "First let's set the required logging level. Set to INFO for displaying basic important logs. Set to DEBUG for full verbosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e1d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Set the log level via environment variable before importing nvidia_rag\n",
    "# This ensures the package respects our log level setting\n",
    "LOGLEVEL = logging.WARNING  # Set to INFO, DEBUG, WARNING or ERROR\n",
    "os.environ[\"LOGLEVEL\"] = logging.getLevelName(LOGLEVEL)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=LOGLEVEL, force=True)\n",
    "\n",
    "# Set log levels for specific loggers after package import\n",
    "for name in logging.root.manager.loggerDict:\n",
    "    if name == \"nvidia_rag\" or name.startswith(\"nvidia_rag.\"):\n",
    "        logging.getLogger(name).setLevel(LOGLEVEL)\n",
    "    if name == \"nv_ingest_client\" or name.startswith(\"nv_ingest_client.\"):\n",
    "        logging.getLogger(name).setLevel(LOGLEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be07e0e",
   "metadata": {},
   "source": [
    "## Import the packages\n",
    "You can import both or either one based on your requirements. `NvidiaRAG()` exposes APIs to interact with the uploaded documents or retrieve summaries and `NvidiaRAGIngestor()` exposes APIs for document upload, management and summary generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db7b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia_rag import NvidiaRAG, NvidiaRAGIngestor\n",
    "from nvidia_rag.utils.configuration import NvidiaRAGConfig\n",
    "from nvidia_rag.rag_server.response_generator import retrieve_summary\n",
    "\n",
    "# Get the configuration object\n",
    "CONFIG = NvidiaRAGConfig.from_yaml(\"config.yaml\")\n",
    "\n",
    "rag = NvidiaRAG(config=CONFIG)\n",
    "ingestor = NvidiaRAGIngestor(config=CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fceebee",
   "metadata": {},
   "source": [
    "### Step 2: View Default Summarizer LLM Settings\n",
    "\n",
    "Let's see what LLM model and parameters are used by default for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DEFAULT SUMMARIZER LLM CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model:             {CONFIG.summarizer.model_name}\")\n",
    "print(f\"Server URL:        {CONFIG.summarizer.server_url}\")\n",
    "print(f\"Temperature:       {CONFIG.summarizer.temperature}\")\n",
    "print(f\"Top P:             {CONFIG.summarizer.top_p}\")\n",
    "print(f\"Max Parallel:      {CONFIG.summarizer.max_parallelization}\")\n",
    "print(f\"Max Chunk Length:  {CONFIG.summarizer.max_chunk_length}\")\n",
    "print(f\"Chunk Overlap:     {CONFIG.summarizer.chunk_overlap}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f697c",
   "metadata": {},
   "source": [
    "### Step 3: View Default Summarization Prompts\n",
    "\n",
    "The prompt template controls how the LLM generates summaries. Let's see the default prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cbb7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia_rag.utils.llm import get_prompts\n",
    "import json\n",
    "# Get all prompts\n",
    "prompts = get_prompts()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DEFAULT DOCUMENT SUMMARY PROMPT\")\n",
    "print(\"=\" * 70)\n",
    "print(json.dumps(prompts[\"document_summary_prompt\"], indent=2))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DEFAULT ITERATIVE SUMMARY PROMPT\")\n",
    "print(\"=\" * 70)\n",
    "print(json.dumps(prompts[\"iterative_summary_prompt\"], indent=2))\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce20869",
   "metadata": {},
   "source": [
    "This will display the default prompts used for:\n",
    "- **document_summary_prompt**: Summarizing a single document or chunk (used for full multimodal extraction)\n",
    "- **shallow_summary_prompt**: Summarizing with fast text-only extraction (used when `shallow_summary: true`)\n",
    "- **iterative_summary_prompt**: Combining multiple summaries for large documents\n",
    "\n",
    "The system automatically selects the appropriate prompt based on extraction mode and document size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ec763e",
   "metadata": {},
   "source": [
    "## Part 2: Library Mode - Change Configuration\n",
    "\n",
    "Now let's see how to modify these settings programmatically in library mode.\n",
    "\n",
    "### Method 1: Change LLM Model and Parameters\n",
    "\n",
    "You can change the model and sampling parameters dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc84ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to a different model (e.g., Llama 3.1 70B)\n",
    "CONFIG.summarizer.model_name = \"meta/llama-3.1-70b-instruct\"\n",
    "CONFIG.summarizer.server_url = \"\"\n",
    "\n",
    "# Lower temperature for more deterministic, focused summaries\n",
    "CONFIG.summarizer.temperature = 0.2\n",
    "\n",
    "# Adjust top_p for nucleus sampling\n",
    "CONFIG.summarizer.top_p = 0.7\n",
    "\n",
    "# Configure global rate limiting (max parallel summary tasks across all workers)\n",
    "# Prevents overwhelming GPU/API with too many concurrent LLM calls\n",
    "CONFIG.summarizer.max_parallelization = 10  # Default: 20\n",
    "\n",
    "print(\"‚úÖ Updated Summarizer Configuration:\")\n",
    "print(f\"   Model:       {CONFIG.summarizer.model_name}\")\n",
    "print(f\"   Server URL:  {CONFIG.summarizer.server_url}\")\n",
    "print(f\"   Temperature: {CONFIG.summarizer.temperature}\")\n",
    "print(f\"   Top P:       {CONFIG.summarizer.top_p}\")\n",
    "print(f\"   Max Parallel:{CONFIG.summarizer.max_parallelization}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dabf4c",
   "metadata": {},
   "source": [
    "### Method 2: Change Summarization Prompt\n",
    "\n",
    "Customize the prompt to change the style and focus of summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05167610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom summary prompt\n",
    "summary_prompt = {\n",
    "    \"system\": \"/no_think\",\n",
    "    \"human\": \"\"\"You are a documentation specialist.\n",
    "\n",
    "Create a clear, summary that:\n",
    "1. Identifies the main topic and purpose\n",
    "2. Lists key concepts or features\n",
    "3. Highlights important procedures or steps  \n",
    "4. Notes any warnings or critical information\n",
    "\n",
    "Keep the summary concise.\n",
    "\n",
    "Text to summarize:\n",
    "{document_text}\n",
    "\n",
    "Summary:\"\"\"\n",
    "}\n",
    "\n",
    "# Get the prompts dictionary and apply the custom prompt\n",
    "from nvidia_rag.utils.llm import get_prompts\n",
    "\n",
    "prompts = get_prompts()\n",
    "prompts[\"document_summary_prompt\"] = summary_prompt\n",
    "\n",
    "print(\"‚úÖ Custom document summary prompt applied\")\n",
    "print(\"\\nNew prompt preview (first 200 chars):\")\n",
    "print(prompts[\"document_summary_prompt\"][\"human\"][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56d2be2",
   "metadata": {},
   "source": [
    "### Method 3: Configure Summary Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce6903",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_options = {\n",
    "    # Page filtering: [[1, 10]] (ranges), [[-5, -1]] (last N pages), \"even\"/\"odd\"\n",
    "    \"page_filter\": [[1, 10]],  # Only pages 1-10\n",
    "    \n",
    "    # Fast mode: Text-only extraction first, summary in seconds\n",
    "    \"shallow_summary\": True,  # Default: False\n",
    "    \n",
    "    # Strategy: None (iterative/best), \"single\" (fastest/truncates), \"hierarchical\" (parallel/faster than iterative)\n",
    "    \"summarization_strategy\": \"hierarchical\"  # Default: None\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"  ‚Ä¢ Page Filter: {summary_options['page_filter']}\")\n",
    "print(f\"  ‚Ä¢ Shallow Summary: {summary_options['shallow_summary']}\")\n",
    "print(f\"  ‚Ä¢ Strategy: {summary_options['summarization_strategy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3be036",
   "metadata": {},
   "source": [
    "### COMPLETE WORKFLOW: Upload ‚Üí Check Status ‚Üí Get Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c75f90",
   "metadata": {},
   "source": [
    "### Create Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection\n",
    "collection_name = \"test_summary\"\n",
    "response = ingestor.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vdb_endpoint=\"http://localhost:19530\"\n",
    ")\n",
    "print(f\"‚úÖ Collection response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5baec",
   "metadata": {},
   "source": [
    "### Uploading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf50562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Upload documents with summary options\n",
    "result = await ingestor.upload_documents(\n",
    "    filepaths=[\"../data/multimodal/functional_validation.pdf\"],\n",
    "    collection_name=collection_name,\n",
    "    generate_summary=True,\n",
    "    summary_options=summary_options,  # From previous cell\n",
    "    blocking=False  # Don't wait, check status instead\n",
    ")\n",
    "print(f\"‚úÖ Upload started: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02afe705",
   "metadata": {},
   "source": [
    "### Checking Status and getting Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cebc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Check summary status\n",
    "status = await retrieve_summary(\n",
    "    collection_name=collection_name,\n",
    "    file_name=\"functional_validation.pdf\",\n",
    "    wait=False  # Just check, don't wait\n",
    ")\n",
    "print(f\"\\nüìä Status: {status.get('status')}\")\n",
    "if status.get('status') == 'IN_PROGRESS':\n",
    "    progress = status.get('progress', {})\n",
    "    print(f\"   Progress: Chunk {progress.get('current')}/{progress.get('total')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f8dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Get summary (blocking - waits until complete)\n",
    "summary_result = await retrieve_summary(\n",
    "    collection_name=collection_name,\n",
    "    file_name=\"functional_validation.pdf\",\n",
    "    wait=True,\n",
    "    timeout=300\n",
    ")\n",
    "\n",
    "if summary_result.get('status') == 'SUCCESS':\n",
    "    print(f\"\\n‚úÖ Summary:\\n{summary_result.get('summary')}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå {summary_result.get('status')}: {summary_result.get('message')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de34199d",
   "metadata": {},
   "source": [
    "### Delete Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe09fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the test collection\n",
    "response = ingestor.delete_collections(\n",
    "    collection_names=[collection_name],\n",
    "    vdb_endpoint=\"http://localhost:19530\"\n",
    ")\n",
    "print(f\"‚úÖ Delete response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caea11bf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55c3b72",
   "metadata": {},
   "source": [
    "## Part 3: Docker Mode - Change Configuration via Environment Variables\n",
    "\n",
    "When running in Docker mode (default), you configure summarization via environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e4a17e",
   "metadata": {},
   "source": [
    "### Method 1: Set Environment Variables\n",
    "\n",
    "Configure the ingestor server by exporting environment variables before startup. Adjust these values according to your requirements:\n",
    "\n",
    "```bash\n",
    "export SUMMARY_LLM=\"meta/llama-3.1-70b-instruct\"\n",
    "export SUMMARY_LLM_SERVERURL=\"\"\n",
    "export SUMMARY_LLM_TEMPERATURE=0.2\n",
    "export SUMMARY_LLM_TOP_P=0.7\n",
    "export SUMMARY_LLM_MAX_CHUNK_LENGTH=9000\n",
    "export SUMMARY_CHUNK_OVERLAP=400\n",
    "export SUMMARY_MAX_PARALLELIZATION=20\n",
    "\n",
    "docker compose -f deploy/compose/docker-compose-ingestor-server.yaml up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7307d496",
   "metadata": {},
   "source": [
    "### Method 2: Custom Prompts in Docker Mode\n",
    "\n",
    "To change prompts in Docker mode, create a custom `prompt.yaml` file and set the `PROMPT_CONFIG_FILE` environment variable.\n",
    "\n",
    "#### Step 1: Create Custom Prompt File\n",
    "\n",
    "Create your custom prompt file (e.g., `/home/user/my_custom_prompt.yaml`):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a835ba2b",
   "metadata": {},
   "source": [
    "\n",
    "```yaml# custom_prompt.yaml\n",
    "document_summary_prompt:\n",
    "  system: |\n",
    "    /no_think\n",
    "  \n",
    "  human: |\n",
    "    You are a technical documentation specialist.\n",
    "    \n",
    "    Create a clear, technical summary that:\n",
    "    1. Identifies the main topic and purpose\n",
    "    2. Lists key technical concepts or features\n",
    "    3. Highlights important procedures or steps\n",
    "    4. Notes any warnings or critical information\n",
    "    \n",
    "    Keep the summary concise and technical.\n",
    "    \n",
    "    Text to summarize:\n",
    "    {document_text}\n",
    "    \n",
    "    Technical Summary:\n",
    "\n",
    "iterative_summary_prompt:\n",
    "  system: |\n",
    "    /no_think\n",
    "  \n",
    "  human: |\n",
    "    You are a technical documentation specialist combining summaries.\n",
    "    \n",
    "    Previous Summary:\n",
    "    {previous_summary}\n",
    "    \n",
    "    New chunk:\n",
    "    {new_chunk}\n",
    "    \n",
    "    Create an updated technical summary combining both.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beffd23e",
   "metadata": {},
   "source": [
    "#### Step 2: Set Environment Variable and Restart\n",
    "\n",
    "For the ingestor-server, set the environment variable:\n",
    "\n",
    "```\n",
    "export PROMPT_CONFIG_FILE=/home/user/my_custom_prompt.yaml\n",
    "\n",
    "# Restart the container (no rebuild needed)\n",
    "docker compose -f deploy/compose/docker-compose-ingestor-server.yaml up -d\n",
    "```\n",
    "**Key Points:**\n",
    "- The service will merge your custom prompts with the defaults\n",
    "- Only the prompts you specify will be overridden - all others remain unchanged\n",
    "- No container rebuild is required, just restart with the new environment variable!\n",
    "\n",
    "For more details, see the prompt customization documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1f8530",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb475d78",
   "metadata": {},
   "source": [
    "### Method 3: Using Ingestor Server APIs (Docker Mode)\n",
    "\n",
    "When running in Docker mode, you interact with the ingestor server via REST APIs. Here's the complete workflow for document summarization using APIs.\n",
    "\n",
    "#### Prerequisites\n",
    "- Ensure ingestor-server and rag-server containers are running\n",
    "- Replace `localhost` with actual IP if hosted on another system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069af221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "!pip install aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c02b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import aiohttp\n",
    "\n",
    "# Setup base configuration\n",
    "INGESTOR_BASE_URL = \"http://localhost:8082\"\n",
    "RAG_BASE_URL = \"http://localhost:8081\"\n",
    "\n",
    "\n",
    "async def print_response(response):\n",
    "    \"\"\"Helper to print API response.\"\"\"\n",
    "    try:\n",
    "        response_json = await response.json()\n",
    "        print(json.dumps(response_json, indent=2))\n",
    "    except aiohttp.ClientResponseError:\n",
    "        print(await response.text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece3f9ed",
   "metadata": {},
   "source": [
    "#### Step 1: Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b9bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def check_health():\n",
    "    \"\"\"Check ingestor server health.\"\"\"\n",
    "    url = f\"{INGESTOR_BASE_URL}/v1/health\"\n",
    "    params = {\"check_dependencies\": \"True\"}\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url, params=params) as response:\n",
    "            await print_response(response)\n",
    "\n",
    "await check_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e52f7",
   "metadata": {},
   "source": [
    "#### Step 2: Create Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2908a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_collection(collection_name: str, embedding_dimension: int = 2048):\n",
    "    \"\"\"Create a collection for document storage.\"\"\"\n",
    "    data = {\n",
    "        \"collection_name\": collection_name,\n",
    "        \"embedding_dimension\": embedding_dimension,\n",
    "        \"metadata_schema\": []\n",
    "    }\n",
    "    \n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(\n",
    "                f\"{INGESTOR_BASE_URL}/v1/collection\", \n",
    "                json=data, \n",
    "                headers=headers\n",
    "            ) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Create collection\n",
    "await create_collection(collection_name=\"test_summary_api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9492a594",
   "metadata": {},
   "source": [
    "#### Step 3: Upload Documents with Summary Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf7cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def upload_with_summary(collection_name: str, filepaths: list):\n",
    "    \"\"\"Upload documents and generate summaries.\"\"\"\n",
    "    \n",
    "    # Configure summary options\n",
    "    data = {\n",
    "        \"collection_name\": collection_name,\n",
    "        \"blocking\": False,  # Non-blocking upload\n",
    "        \"split_options\": {\"chunk_size\": 512, \"chunk_overlap\": 150},\n",
    "        \"generate_summary\": True,  # Enable summary generation\n",
    "        \"summary_options\": {\n",
    "            \"page_filter\": [[1, 10], [-5, -1]],  # First 10 and last 5 pages\n",
    "            \"shallow_summary\": True,  # Fast text-only extraction\n",
    "            \"summarization_strategy\": \"single\"  # fastest strategy other available: \"hierarchical\", None(iterative)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    form_data = aiohttp.FormData()\n",
    "    for file_path in filepaths:\n",
    "        form_data.add_field(\n",
    "            \"documents\",\n",
    "            open(file_path, \"rb\"),\n",
    "            filename=os.path.basename(file_path),\n",
    "            content_type=\"application/pdf\",\n",
    "        )\n",
    "    \n",
    "    form_data.add_field(\"data\", json.dumps(data), content_type=\"application/json\")\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(\n",
    "                f\"{INGESTOR_BASE_URL}/v1/documents\", \n",
    "                data=form_data\n",
    "            ) as response:\n",
    "                await print_response(response)\n",
    "                response_json = await response.json()\n",
    "                return response_json.get(\"task_id\")\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return None\n",
    "\n",
    "# Upload documents\n",
    "task_id = await upload_with_summary(\n",
    "    collection_name=\"test_summary_api\",\n",
    "    filepaths=[\"../data/multimodal/functional_validation.pdf\"]\n",
    ")\n",
    "print(f\"\\n‚úÖ Upload task_id: {task_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c4704",
   "metadata": {},
   "source": [
    "#### Step 4: Check Upload Status (Ingestor Server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d17b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def check_upload_status(task_id: str):\n",
    "    \"\"\"Check ingestion task status.\"\"\"\n",
    "    params = {\"task_id\": task_id}\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.get(\n",
    "                f\"{INGESTOR_BASE_URL}/v1/status\", \n",
    "                params=params, \n",
    "                headers=headers\n",
    "            ) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Check status\n",
    "if task_id:\n",
    "    await check_upload_status(task_id=task_id)\n",
    "else:\n",
    "    print(\"No task_id available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9986e2f9",
   "metadata": {},
   "source": [
    "#### Step 5: Check Summary Status (RAG Server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b773bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def check_summary_status(collection_name: str, file_name: str):\n",
    "    \"\"\"Check summary generation status via RAG server.\"\"\"\n",
    "    params = {\n",
    "        \"collection_name\": collection_name,\n",
    "        \"file_name\": file_name,\n",
    "        \"blocking\": \"false\"  # Just check status, don't wait\n",
    "    }\n",
    "    \n",
    "    url = f\"{RAG_BASE_URL}/v1/summary\"\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.get(url, params=params) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Check summary status\n",
    "await check_summary_status(\n",
    "    collection_name=\"test_summary_api\",\n",
    "    file_name=\"functional_validation.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb0e82b",
   "metadata": {},
   "source": [
    "#### Step 6: Docker/API Mode - Delete Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7edca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def delete_collections(collection_names: list[str]):\n",
    "    \"\"\"Delete collections from the vector store.\"\"\"\n",
    "    url = f\"{INGESTOR_BASE_URL}/v1/collections\"\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.delete(url, json=collection_names) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Delete the test collection\n",
    "await delete_collections(collection_names=[\"test_summary_api\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab41ef",
   "metadata": {},
   "source": [
    "## Summary of Available Configuration Options\n",
    "\n",
    "### Summarizer Configuration Fields\n",
    "\n",
    "| Field | Environment Variable | Default Value | Description |\n",
    "|-------|---------------------|---------------|-------------|\n",
    "| `model_name` | `SUMMARY_LLM` | `nvidia/llama-3.3-nemotron-super-49b-v1.5` | The LLM model used for summarization |\n",
    "| `server_url` | `SUMMARY_LLM_SERVERURL` | (empty) | Server URL for custom model hosting |\n",
    "| `temperature` | `SUMMARY_LLM_TEMPERATURE` | `0.0` | Controls randomness (0.0-1.0) |\n",
    "| `top_p` | `SUMMARY_LLM_TOP_P` | `1.0` | Nucleus sampling parameter (0.0-1.0) |\n",
    "| `max_chunk_length` | `SUMMARY_LLM_MAX_CHUNK_LENGTH` | `9000` | Maximum chunk size in tokens |\n",
    "| `chunk_overlap` | `SUMMARY_CHUNK_OVERLAP` | `400` | Overlap between chunks in tokens |\n",
    "\n",
    "### Prompt Template Variables\n",
    "\n",
    "- **document_summary_prompt**: Use `{document_text}` variable\n",
    "- **iterative_summary_prompt**: Use `{previous_summary}` and `{new_chunk}` variables\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** Changes made in library mode take effect immediately without restarting any services. Changes in Docker mode require a container restart but no rebuild."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
